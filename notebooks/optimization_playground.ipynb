{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "playground.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4J1lmK2Viqey"
   },
   "source": [
    "# Text-Guided Editing of Images (Using CLIP and StyleGAN)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NZzVvsrKivwl",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "cellView": "form",
    "outputId": "b6d100d4-606e-4d78-ee63-6c0f16ca423b"
   },
   "source": [
    "#@title Setup (may take a few minutes)\n",
    "!git clone https://github.com/orpatashnik/StyleCLIP.git\n",
    "\n",
    "import os\n",
    "os.chdir(f'./StyleCLIP')\n",
    "\n",
    "!pip install ftfy regex tqdm\n",
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# Authenticate and create the PyDrive client.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "file_id = '1EM87UquaoQmk17Q8d5kYIAHqu0dkYqdT'\n",
    "downloaded = drive.CreateFile({'id':file_id})\n",
    "downloaded.FetchMetadata(fetch_all=True)\n",
    "downloaded.GetContentFile(downloaded.metadata['title'])"
   ],
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'StyleCLIP'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ftfy\n",
      "  Using cached ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "Requirement already satisfied: regex in c:\\users\\marti\\anaconda3\\lib\\site-packages (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\marti\\anaconda3\\lib\\site-packages (4.64.1)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in c:\\users\\marti\\anaconda3\\lib\\site-packages (from ftfy) (0.2.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\marti\\anaconda3\\lib\\site-packages (from tqdm) (0.4.5)\n",
      "Installing collected packages: ftfy\n",
      "Successfully installed ftfy-6.1.1\n",
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to c:\\users\\marti\\appdata\\local\\temp\\pip-req-build-gri7y1xe\n",
      "  Resolved https://github.com/openai/CLIP.git to commit a9b1bf5920416aaeaec965c25dd9e8f98c864f16\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: ftfy in c:\\users\\marti\\anaconda3\\lib\\site-packages (from clip==1.0) (6.1.1)\n",
      "Requirement already satisfied: regex in c:\\users\\marti\\anaconda3\\lib\\site-packages (from clip==1.0) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\marti\\anaconda3\\lib\\site-packages (from clip==1.0) (4.64.1)\n",
      "Requirement already satisfied: torch in c:\\users\\marti\\anaconda3\\lib\\site-packages (from clip==1.0) (1.7.1)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.14.1-cp37-cp37m-win_amd64.whl (1.1 MB)\n",
      "     ---------------------------------------- 1.1/1.1 MB 4.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in c:\\users\\marti\\anaconda3\\lib\\site-packages (from ftfy->clip==1.0) (0.2.5)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\marti\\anaconda3\\lib\\site-packages (from torch->clip==1.0) (4.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\marti\\anaconda3\\lib\\site-packages (from torch->clip==1.0) (1.21.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\marti\\anaconda3\\lib\\site-packages (from torchvision->clip==1.0) (9.2.0)\n",
      "Requirement already satisfied: requests in c:\\users\\marti\\anaconda3\\lib\\site-packages (from torchvision->clip==1.0) (2.28.1)\n",
      "Collecting torch\n",
      "  Downloading torch-1.13.1-cp37-cp37m-win_amd64.whl (162.6 MB)\n",
      "     -------------------------------------- 162.6/162.6 MB 3.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\marti\\anaconda3\\lib\\site-packages (from tqdm->clip==1.0) (0.4.5)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\marti\\anaconda3\\lib\\site-packages (from requests->torchvision->clip==1.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\marti\\anaconda3\\lib\\site-packages (from requests->torchvision->clip==1.0) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\marti\\anaconda3\\lib\\site-packages (from requests->torchvision->clip==1.0) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\marti\\anaconda3\\lib\\site-packages (from requests->torchvision->clip==1.0) (3.3)\n",
      "Building wheels for collected packages: clip\n",
      "  Building wheel for clip (setup.py): started\n",
      "  Building wheel for clip (setup.py): finished with status 'done'\n",
      "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369447 sha256=83f6f14a1a648e94a80f70a966b7fe903fd5e8a16aeb2830664f493bb784c6d0\n",
      "  Stored in directory: C:\\Users\\marti\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-em4s1hsh\\wheels\\fd\\b9\\c3\\5b4470e35ed76e174bff77c92f91da82098d5e35fd5bc8cdac\n",
      "Successfully built clip\n",
      "Installing collected packages: torch, torchvision, clip\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.7.1\n",
      "    Uninstalling torch-1.7.1:\n",
      "      Successfully uninstalled torch-1.7.1\n",
      "Successfully installed clip-1.0 torch-1.13.1 torchvision-0.14.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git 'C:\\Users\\marti\\AppData\\Local\\Temp\\pip-req-build-gri7y1xe'\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pydrive'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_2640\\3087412533.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[0mget_ipython\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msystem\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'pip install git+https://github.com/openai/CLIP.git'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 10\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mpydrive\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mauth\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mGoogleAuth\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     11\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mpydrive\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdrive\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mGoogleDrive\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mgoogle\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcolab\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mauth\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'pydrive'"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XTAVTULlq87j",
    "cellView": "form"
   },
   "source": [
    "experiment_type = 'edit' #@param ['edit', 'free_generation']\n",
    "\n",
    "description = 'A person with purple hair' #@param {type:\"string\"}\n",
    "\n",
    "latent_path = None #@param {type:\"string\"}\n",
    "\n",
    "optimization_steps = 100 #@param {type:\"number\"}\n",
    "\n",
    "l2_lambda = 0.008 #@param {type:\"number\"}\n",
    "\n",
    "create_video = True #@param {type:\"boolean\"}"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CcBz_eEomF7Q",
    "cellView": "form"
   },
   "source": [
    "#@title Additional Arguments\n",
    "args = {\n",
    "    \"description\": description,\n",
    "    \"ckpt\": \"stylegan2-ffhq-config-f.pt\",\n",
    "    \"stylegan_size\": 1024,\n",
    "    \"lr_rampup\": 0.05,\n",
    "    \"lr\": 0.1,\n",
    "    \"step\": optimization_steps,\n",
    "    \"mode\": experiment_type,\n",
    "    \"l2_lambda\": l2_lambda,\n",
    "    \"latent_path\": latent_path,\n",
    "    \"truncation\": 0.7,\n",
    "    \"save_intermediate_image_every\": 1 if create_video else 20,\n",
    "    \"results_dir\": \"results\"\n",
    "}"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WT9JRl8hnT1l",
    "outputId": "9b64415b-6854-4159-fc1f-57b0768fad99"
   },
   "source": [
    "from optimization.run_optimization import main\n",
    "from argparse import Namespace\n",
    "result = main(Namespace(**args))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 529
    },
    "id": "h15xcbHwnW0U",
    "cellView": "form",
    "outputId": "b6cb5446-e7ec-4872-ee7a-78ca0717f7c3"
   },
   "source": [
    "#@title Visualize Result\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.transforms import ToPILImage\n",
    "result_image = ToPILImage()(make_grid(result.detach().cpu(), normalize=True, scale_each=True, range=(-1, 1), padding=0))\n",
    "h, w = result_image.size\n",
    "result_image.resize((h // 2, w // 2))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lNRSSi0IcQID",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 870
    },
    "cellView": "form",
    "outputId": "a64d5598-41d0-48ef-c427-9323b7a74a38"
   },
   "source": [
    "#@title Create and Download Video\n",
    "\n",
    "!ffmpeg -r 15 -i results/%05d.png -c:v libx264 -vf fps=25 -pix_fmt yuv420p out.mp4\n",
    "from google.colab import files\n",
    "files.download('out.mp4')"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}
